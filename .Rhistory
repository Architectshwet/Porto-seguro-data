library(dplyr) #data manipulation
library(readr) #input/output
library(data.table) #data manipulation
library(stringr) #string manipulation
library(caret)  #model evaluation (confusion matrix)
library(tibble) #data wrangling
library("ROSE") #over/under sampling
library("randomForest") #random forest model building
library(pROC) #ROC plots
library("MLmetrics") #Normalized Gini
train = as.tibble(fread("train.csv",na.strings = c("-1","-1.0"))) #given train data
test = as.tibble(fread("test.csv",na.strings = c("-1","-1.0"))) #given test data
#Loading the given data
setwd("C:/Users/Architect_shwet/Desktop/New folder/insurance")
train = as.tibble(fread("train.csv",na.strings = c("-1","-1.0"))) #given train data
test = as.tibble(fread("test.csv",na.strings = c("-1","-1.0"))) #given test data
#str(train);str(test)
dim(train);dim(test) #dimensions of train and test
table(train$target) #examining the target variable
test$target = 0 #creating a target variable in test data
test$data = "test" #creating another variable to identify the test data rows
test = test[, c(1, 60, 59, 2:58)] #reforming with newly created variables
train$data = "train" #creating another variable to identify the train data rows
train = train[, c(1, 60, 2:59)] #reforming with newly created variables
combined_data = as.data.frame(rbind(train,test)) #combining test and train data
dim(combined_data) #dimensions of combined data
combined_data <- combined_data %>%
mutate_at(vars(ends_with("cat")), funs(as.factor)) %>%
mutate_at(vars(ends_with("bin")), funs(as.logical)) %>%
mutate(target = as.factor(target))
str(combined_data)
#dataframe of missing values column wise
missing_values = as.data.frame(colSums(is.na(combined_data)))
missing_values
#vectordrop is a vector with columns having more than 5% of missing values
vectordrop <- combined_data[, lapply( combined_data,
function(m) sum(is.na(m)) / length(m) ) >= .05 ]
#removing the columns in vectordrop from the main data
combined_data = combined_data[,!(colnames(combined_data) %in% colnames(vectordrop))]
dim(combined_data)
miss_pct <- sapply(combined_data, function(x) { sum(is.na(x)) / length(x) })
miss_pct <- miss_pct[miss_pct > 0]
names(miss_pct) #columns with missing data
#Designing a function to impute factor columns with mode.
mode <- function (x, na.rm) {
xtab <- table(x)
xmode <- names(which(xtab == max(xtab)))
if (length(xmode) > 1) xmode <- ">1 mode"
return(xmode)
}
df = combined_data #just making typing easy :P
rm(combined_data) #saving RAM
df$ps_ind_02_cat[is.na(df$ps_ind_02_cat)]<-mode(df$ps_ind_02_cat) #imputing with mode
df$ps_ind_04_cat[is.na(df$ps_ind_04_cat)]<-mode(df$ps_ind_04_cat)
df$ps_ind_05_cat[is.na(df$ps_ind_05_cat)]<-mode(df$ps_ind_05_cat)
df$ps_car_01_cat[is.na(df$ps_car_01_cat)]<-mode(df$ps_car_01_cat)
df$ps_car_02_cat[is.na(df$ps_car_02_cat)]<-mode(df$ps_car_02_cat)
df$ps_car_07_cat[is.na(df$ps_car_07_cat)]<-mode(df$ps_car_07_cat)
df$ps_car_09_cat[is.na(df$ps_car_09_cat)]<-mode(df$ps_car_09_cat)
df$ps_car_11[is.na(df$ps_car_11)]<-mode(df$ps_car_11)
df$ps_car_12[is.na(df$ps_car_12)]<-mean(df$ps_car_12,na.rm=T) #imputing with mean
#checking for missing values
sum(is.na(df))
TRAIN <- df[1:595212,-2] #train data set after pre-processing
dim(TRAIN)
TEST <- df[595213:1488028,-c(2,3)] #test data set after pre-processing
dim(TEST)
table(TRAIN$target)
table(TRAIN$target)/nrow(TRAIN) #Here we observe that target variable is inbalanced,
#So,for better model to be build target variable should be balanced.
#we using over and undersampling from ROSE package for this
balanced_train <- ovun.sample(target~.,data=TRAIN,method = "both",N = 90000,p=.5,seed=1)$data
head(balanced_train) #head of the dataset with balanced target
dim(balanced_train) #dimensions of the dataset with balanced target.
sum(is.na(balanced_train)) #checking for any Missing values
balanced_train <-as.data.frame(balanced_train)
table(balanced_train$target) #analyzing the target variable in this dataset
table(balanced_train$target)/nrow(balanced_train) # Now data is balanced
str(balanced_train)
#Now we split above sample data into train and test data ##
s=sample(nrow(balanced_train),round(nrow(balanced_train)*0.7),replace=FALSE)
train = balanced_train[s,]
test = balanced_train[-s,]
dim(train);dim(test)
#Random forest will not take a variable with more than 53 categories,
#so we remove variable "ps_car_11_cat"
unique(train$ps_car_11_cat)
dim(train)
sapply(train,class)
train_rf = subset(train,select=-ps_car_11_cat)
#rm(train) #train without ps_car_11_cat
test_rf = subset(test,select=-ps_car_11_cat)
library("randomForest")
model_rf = randomForest(as.factor(target) ~. , data = train_rf) # Fit Random forest
pred_rf <- predict(model_rf,test_rf) #predictin using the model
#accuracy of the model
confusionMatrix(pred_rf,test_rf$target)
d = importance(model_rf)
d
names(train_rf)
table(train_rf$id)
#creating dataset with imp variable
train_with_imp = subset(train_rf,select=c(1,(which(d>300))+1))
test_with_imp = subset(test_rf,select=c(1,(which(d>300))+1))
#model with important variables
model_rf2 <- randomForest(as.factor(target) ~. , data = train_with_imp,keep.forest=TRUE)
# Prediction for  Original TEST data
#TEST_rf = TEST[,-ps_car_11_cat]
pred_rf_TEST <- predict(model_rf2,TEST,type="prob")
predicted <- ifelse(pred_rf_TEST > 0.5, 1,0)
#Final submission file in mentioned format
preds <- data.table(id = TEST$id ,target = predicted[,2])
write.table(preds, "submission.csv", sep=",", dec=".", quote=FALSE, row.names=FALSE)
head(preds,5)
#Final submission file in mentioned format
preds <- data.table(id = TEST$id ,target = pred_rf_TEST[,2])
write.table(preds, "submission.csv", sep=",", dec=".", quote=FALSE, row.names=FALSE)
head(preds,5)
